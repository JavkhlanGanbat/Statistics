---
lang: mn
nocite: '@*'
format:
  pdf:
    toc: false
    include-in-header:
      text: |
        \usepackage{graphicx}
        \usepackage{geometry}
        \usepackage{enumitem}
        \setlist[itemize]{leftmargin=*}
  html:
    toc: true
---
\begin{titlepage}
\centering
{\Large \textbf{МОНГОЛ УЛСЫН ИХ СУРГУУЛЬ\\
МЭДЭЭЛЛИЙН ТЕХНОЛОГИ, ЭЛЕКТРОНИКИЙН СУРГУУЛЬ}}\\[2cm]

\includegraphics[width=0.4\textwidth]{images/numlogo.png}\\[1.5cm]

{\Large\bfseries Магадлал статистик\par}
{\Huge\bfseries Логистик регресс ашиглан орлогын түвшинг илрүүлэх}\\[2cm]

\raggedright

\begin{tabular}{@{}l l}
{\large \textbf{Шалгасан:}} & \hspace{8cm} Г. Махгал (Ахлах багш) \\[1cm]

{\large \textbf{Гүйцэтгэсэн:}} & \hspace{8cm} Г. Жавхлан (22b1num3154) \\
                               & \hspace{8cm} Б. Солонгоо (23b1num1034) \\
                               & \hspace{8cm} Б. Балжинням (22b1num6983) \\
                               & \hspace{8cm} Т. Баасандорж (22b1num0004) \\
\end{tabular}



% Доод талд байршил ба огноо
\vfill
\centering
{\large Улаанбаатар}\\
{\large \today\par}

\end{titlepage}

\newpage
\hypersetup{
  colorlinks=true,
  linkcolor=black,
  urlcolor=blue,
  citecolor=blue
}
\renewcommand{\contentsname}{Агуулга}
\tableofcontents

\newpage

# Оршил

Энэ төслийн зорилго нь хувь хүний мэдээллийг нь ашиглан жилийн орлого нь 50,000 ам.доллароос дээш эсэхийг таамаглах явдал юм. Бид Kaggle платформ дээрх Adult Income өгөгдлийг ашиглан орлогын түвшинг урьдчилан таамаглах логистик регресс загвар боловсруулж, гаргасан үр дүн дээр дүн шинжилгээ хийнэ. Үүний зэрэгцээ загварынхаа магадлалт болон статистикийн суурьтай танилцаж, оновчлолын аргуудын талаар сурах болно.

# Өгөгдөл

## Өгөгдлийн эх үүсвэр ба зорилго

Бидний ашигласан өгөгдөл нь 1994 U.S. Census Current Population Survey судалгаанаас гаргаж авсан бодит өгөгдлийн цэвэрлэгдсэн хэсэг юм. Энэхүү цуглуулгыг Барри Беккер боловсруулж, Ронни Кохави UCI-ийн машин сургалтын агуулахад хандивласан. Энэ өгөгдлийн багцад нас, боловсрол, мэргэжил, гэрлэлтийн байдал, хүйс зэрэг нийгэм эдийн засгийн шинж чанаруудыг илэрхийлэх олон хувьсагч орсон. Бидний зорилтот буюу хамааран хувьсагч бол `income_>50K` юм. Өгөгдлийг цэвэрлэсний дараа үлдсэн хувьсагчдын мэдээллийг ашиглан энэ хувьсагчийн утгыг зөв таамаглах нь бидний үндсэн зорилго болно.

Түүврийн нийт хэмжээ нь ойролцоогоор 44,000 бөгөөд бид үүнийг сургах болон үнэлгээ хийх хоёр хэсэгт хуваана. Хуваалтын дараа 35165 мөр нь сургалтын хэсэг, 8792 мөр нь баталгаажуулалт, шалгалтын хэсгийг бүрдүүлнэ.

Гэвч өгөгдөлтэй ажиллах гол бэрхшээл нь классуудын тэнцвэргүй байдал юм. Эх олонлогоос ойролцоогоор 76% хүмүүс нь $\leq50K$ орлоготой. Харин 24% нь $>50K$ орлоготой.

Өгөгдөлд хүмүүсийн ихэнх хувь нь 50K-аас бага орлоготой байгаа тул энгийн загвар ашиглавал accuracy нь 76% гарна гэсэн үг. Гэвч энэ 50K-аас бага орлоготой хүмүүсийг огт харгалзан үзэхгүй тул хангалтгүй юм. Тиймээс бид 2 классыг хоёуланг нь оновчтойгоор авч үздэг загварыг бүтээж, аль аль классыг нь зөв таамаглах боломжтой загвар боловсруулна. Үүний тулд F1 оноо, recall зэрэг үзүүлэлтүүдийг чухалчилна.

## Өгөгдлөө цэвэрлэх

**Өгөгдлийн баганууд:**

[`age`, `workclass`, `fnlwgt`, `education`, `educational-num`, `marital-status`, `occupation`,  `relationship`, `race`, `gender`, `capital-gain`, `capital-loss`, `hours-per-week`, `native-country`, `income_>50K`]

Анхны өгөгдлийг шалгасны дараа бид давхардсан, ач холбогдол багатай, болон тайлбарлахад хэцүү багануудыг арилгаж, өгөгдлийг хялбаршуулсан.

**Ашигласан хувьсагчид (9):**

- Тоон хувьсагч (5): `age`, `educational-num`, `capital-gain`, `capital-loss`, `hours-per-week`

- Чанарын хувьсагч (4): `education`, `marital-status`, `occupation`, `gender`

## Өгөгдөл хуваах

Бид өгөгдлийг сургалт болон баталгаажуулалт гэсэн хоёр хэсэгт **80/20** харьцаатайгаар хуваасан. Ингэхдээ income_>50K классын харьцаа тэнцвэртэй байхаар хуваасан бөгөөд үр дүнд нь дараах 2 файлыг үүсгэж хадгалсан:

- Сургалтын багц: `train_split.csv`
- Баталгаажуулалтын багц: `val_split.csv`

Анхны өгөгдлийн багц дотор `income_>50K` хувьсагчийн 76% нь 0, 24% нь 1 утгатай байсан бол хуваалтын дараа энэ харьцаа эвдрээгүй, хэвээрээ үлдсэн.

```python
# Өгөгдлийг унших
df = pd.read_csv("data.csv")

# Ашиглах хувьсагчид
keep_cols = ["age", "educational-num", "capital-gain", "capital-loss",
             "hours-per-week", "education", "marital-status", "occupation",
             "gender", "income_>50K"]

df = df[keep_cols]

# Сургалт/баталгаажуулалт хуваалт
train_df, val_df = train_test_split(
    df, test_size=0.2, random_state=42, stratify=df["income_>50K"]
)

# Файлуудыг хадгалах
train_df.to_csv("train_split.csv", index=False)
val_df.to_csv("val_split.csv", index=False)

```
## Урьдчилсан боловсруулалт

Логистик регресс загварт өгөгдлийг оруулахын өмнө тоон болон чанарын хувьсагчийг зохих хэлбэрт хөрвүүлэх шаардлагатай. Энэ процессыг дараах байдлаар хийсэн.

**Тоон хувьсагчид:**  

StandardScaler ашиглаж тоон хувьсагч бүрийн дундаж утгыг 0, стандарт хазайлтыг 1 болгож нормальчилсан. Учир нь логистик регресс нь градиент дээр суурилсан алгоритм тул хувьсагчийн тархалт, хэмжээнээс хамаарч удаан суралцаж, тогтворгүй байж болно. Иймд хувьсагчийг түүврийн дундаж руу нь төвлөрүүлснээр сургалтын эрчим, тогтвортой байдал нэмэгдэнэ.

$$
X_{\text{scaled}} = \frac{X - \mu}{\sigma}
$$

**Чанарын хувьсагчид:**  

OneHotEncoder ашиглаж чанарын хувьсагч бүрийг хоёртын вектор болгон хөрвүүлсэн. Учир нь машин сургалтын алгоритмууд текст буюу чанарын утгыг шууд ойлгох боломжгүй тул тоон хэлбэрт хөрвүүлэх шаардлагатай байдаг. OneHotEncoder нь хувьсагчийн утгуудыг ямар нэгэн дараалал, зэрэглэлтэй гэж үзэхгүй.

Жишээлбэл, `education` гэх чанарын хувьсагчийг авч үзье. Үүний утгууд нь:

$$
education = \{\text{HS}, \text{Bachelors}, \text{Masters}, \text{Doctorate}\}
$$

One-hot кодчилол нь уг хувьсагчийг дараах байдлаар хувиргана:

$$
f(x) = \begin{cases} [1, 0, 0, 0] & \text{if } x = \text{HS} \\[1mm][0, 1, 0, 0] & \text{if } x = \text{Bachelors} \\[1mm][0, 0, 1, 0] & \text{if } x = \text{Masters} \\[1mm][0, 0, 0, 1] & \text{if } x = \text{Doctorate} \end{cases}
$$

Эндээс бид онцлогийнхоо (feature) матрицийг гаргаж авна:

$$
X \in \mathbb{R}^{m \times n} =
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1n} \\
x_{21} & x_{22} & \cdots & x_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
x_{m1} & x_{m2} & \cdots & x_{mn}
\end{bmatrix}
$$

Энд:

- $m$ = Түүврийн хэмжээ

- $n$ = Онцлогийн тоо

**Загварын гиперпараметр:**

- `learning_rate` ойролцоогоор 0.1

- `max_iter` ойролцоогоор 1000

- `reg_lambda` = 1e-4 (L2)

- `lr_decay` = 1e-4

- `threshold` = 0.5 (шийдвэрийн хязгаар)

# Загварын хэрэгжүүлэлт

Бид энэ төсөлд логистик регрессийн загварыг гараар хэрэгжүүлсэн. Яагаад гэвэл sklearn-ийн `LogisticRegression` нь олон зүйлийг автоматаар хийдэг бөгөөд бид хэрхэн ажилладгийг нь ойлгохыг илүүд үзлээ. Мөн сурах эрчмийн бууралт, class weighting зэрэг сонирхолтой зүйлсийг өөрсдөө туршиж үзэхийг хүссэн.

## Логистик регрессийн үндэс
 
Логистик регресс нь хоёртын ангилал хийх суурь загваруудын нэг юм. Шугаман регресс нь тасралтгүй утгуудыг таамагладаг бол логистик регресс нь аливаа инстанц нь тодорхой классын гишүүн байх магадлалыг тооцоолдог.
 
Оролтын хувьсагч ${x}$ -н хувьд
$$
P(y = 1 \mid {x})
$$
 
буюу гаралт $y$ нь $1$-тэй тэнцүү байх магадлалыг олох зорилготой.
 
Энэ төслийн хувьд бидний зорилтод хувьсагч маань `income > 50K` үед `1`, `income` $\leq50K$ үед $0$ гэсэн хоёр л утга авна. Иймээс түүвэр болгоны хувьд энэ хувьсагчийг Бернуллийн санамсаргүй хувьсагчаар загварчилж болно.
 
$$
y_i \sim \text{Bernoulli}(p_i)
$$
 
Энд

- $y_i$ = i-р түүвэр дээр ажиглагдсан утга 

- $p_i$ = $P(y_i = 1 \mid {x}_i)$ = i-р түүвэр класс 1-ийн гишүүн байх магадлал

Нэг түүврийн тархалт нь:

$$
P(y_i \mid p_i) = p_i^{y_i}(1 - p_i)^{1 - y_i}.
$$

Одоо бид гарт оролтын онцлог ${x}_i$-с класс 1-ийн магадлалыг илэрхийлэх функц хэрэгтэй байна.

Эхлээд бид оролтуудын шугалан тэгшитгэлийг бодно:
 
$$
z_i = {w}^\top {x}_i + b
$$
 
Энд:
 
- ${w}$ = жингийн вектор (сурах параметрүүд)
- $b$ = хазайлтын утга
- ${x}_i$ = онцлогийн вектор $i$
 
```python
if issparse(X_array):
    z = X_array.dot(self.weights) + self.bias
else:
    z = np.dot(X_array, self.weights) + self.bias
```
## Сигмойд функц
 
Гэхдээ дээрх шугаман тэгшитгэл нь ямар ч жинхэнэ утгыг өгөх боломжтой. Бид гарсан тэр жинхэнэ утгыг $[0,1]$ хооронд орших магадлал болгох хэрэгтэй. Үүний тулд доорх сигмойд функцээр илэрхийлэгдэх логистик муруйг ашиглана.
 
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

```python
def sigmoid(self, z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # overflow-оос сэргийлэх
```

Эндээс модел нэг түүврийн хувьд таамаглах магадлал нь дараах томъёгоор илэрхийлэгдэнэ:

$$
\hat{p}_i = P(y_i = 1 \mid {x}_i) = \sigma(z_i) = \sigma({w}^\top {x}_i + b)
$$

## Эх олонгогийн үнэний хувь

Бүх $y_i$ нь Бернуллийн санамсаргүй утга. Өгөгдлийн цэгүүд нь бусдаасаа хамааргүй гэж үзвэл
$w, b$ хоёр параметрүүдийг өгсөн үед бүх $y_i$-г ажиглах магадлал нь дараах үнэний хувийн функцээр (likelihood function) тооцоологдно:

$$
\mathcal{L}(w,b) = \prod_{i=1}^{m} \hat{p}_i^{y_i} (1 - \hat{p}_i)^{1-y_i}
$$

Энд:

- $y$ = жинхэнэ утга (0 or 1)
- $\hat{p}$ = таамагласан утга

Үнэний хувийн функц нь $w, b$ хоёр параметрүүдийг өгсөн үед бидний эх олонлог ажиглагдах магадлал хэд вэ гэдгийг олж болно гэсэн үг. Энд хамгийн их үнэний хувь бүхий үнэлгээг (maximum likelihood estimation) хийснээр хамгийн оновчтой параметрүүдийг олно.

## Үнэний хувиар алдааг илэрхийлэх

Хэрвээ бид үржвэрүүдтэй ажиллах бол компьютер дүрслэх боломжгүй бага тоотой ажиллах хэрэгтэй болох тул үнэний хувийн функц $\mathcal{L}(w,b)$-н натурал логарифмийг авна.

$$
\log \mathcal{L}(w,b) = \sum_{i=1}^{m}
\left[
y_i \log(\hat{p}_i) + (1-y_i)\log(1 - \hat{p}_i)
\right]
$$

Энэнийг алдааны функц болгохын тулд $-1$-р үржүүлнэ. Өөрөөр хэлбэл binary cross-entropy loss фунцкээ гаргаж ирлээ.

$$
L_{\text{CE}}(w,b) = 
-\sum_{i=1}^{m}
\left[
y_i \log(\hat{p}_i)
+ (1-y_i)\log(1-\hat{p}_i)
\right]
$$

## Binary cross-entropy

Нэг түүврийн алдааг олох томъёо:

$$
\ell = -\left[y\log(\hat{p}) + (1-y)\log(1-\hat{p})\right].
$$

буюу:

- $if$ $y = 1\: \ell = -\log(\hat{p})$

- $if$ $y = 0\: \ell = -\log(1 - \hat{p})$

Cross-entropy нь загварыг итгэлтэйгээр буруу таамаглал гаргахыг илүү шийтгэдэг. Өөрөөр хэлбэл 1 эсвэл 0-тэй маш ойрхон магадлал (0.99, 0.01 г.м.) таамаглахад буруу болж таарвал алдаа нь өндөр гарч ирнэ.

| Таамаглал $\hat{p}$ | Алдаа $-\log(\hat{p})$ | Шийтгэлийн түвшин |
|---------------------|----------------------|----------------|
| 0.99 (итгэлтэй, зөв) | 0.01 | Маш бага |
| 0.50 (итгэл багатай) | 0.69 | Дунд зэргийн |
| 0.10 (итгэлтэй, буруу) | 2.30 | Том |
| 0.01 (маш итгэлтэй, буруу) | 4.61 | Маш том|

$$
L_{\text{CE}} = -\frac{1}{m}\sum_{i=1}^{m} 
\left[
y_i \log(\hat{p}_i) + (1 - y_i)\log(1 - \hat{p}_i)
\right].
$$

## L2 тогтворжуулалт (Regularization)

Хэт том утгатай жингээс үүдэлтэй overfitting-ээс сэргийлнэ 

$$
\frac{\lambda}{2m}\|{w}\|^2
$$

Энд:

- $\lambda$ = тогтворжуулалтын хүч (гиперпараметр)

- $m$ = түүврийн хэмжээ

- $\|{w}\|^2 = w_1^2 + w_2^2 + \cdots + w_n^2$

Нийт алдаа:

$$
L = -\frac{1}{m}\sum_{i=1}^{m}\left[y_i\log(\hat{p}_i) + (1-y_i)\log(1-\hat{p}_i)\right] + \frac{\lambda}{2m}\|{w}\|^2
$$

```python
def compute_loss(self, y_true, y_pred, sample_weights=None):

    m = len(y_true)
    
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    sample_losses = -(y_true * np.log(y_pred) + 
                      (1 - y_true) * np.log(1 - y_pred))
    
    if sample_weights is not None:
        sample_losses = sample_losses * sample_weights
    
    cross_entropy = np.mean(sample_losses)
    
    l2_penalty = (self.reg_lambda / (2 * m)) * np.sum(self.weights ** 2)
    
    return cross_entropy + l2_penalty
```

## Градиент бууруулалт (Gradient descent)

Одоо манай алдааны функц тодорхойлогдсон. Гэхдээ нийт алдааг багасгахын тулд градиент бууруулалт хэмээх аргыг ашиглана. Градиент нь ерөнхийдөө жин болон хазайлтыг аль чиглэлд өөрчлөх үед алдаа нь хамгийн хурдтай өсөж буурахыг илэрхийлэх вектор. Алдааны функцын w, b хоёрын хувьд авсан тухайн уламжлалыг олно.

Жингийн градиент:

$$
\frac{\partial L}{\partial {w}} = \frac{1}{m} {X}^\top (\hat{{p}} - {y}) + \frac{\lambda}{m} {w}
$$

- ${X} \in \mathbb{R}^{m \times n}$: онцлогийн матриц ($m \times n$)
- ${w} \in \mathbb{R}^{n \times 1}$: жингийн вектор 
- $\hat{{p}} \in \mathbb{R}^{m \times 1}$: түүвэр болгоны таамагласан утга 
- ${y} \in \mathbb{R}^{m \times 1}$: жинхэнэ утгууд 
- $\lambda$: тогворжуулалтын хүч  

Жингийн градиент нь $n \times 1$ вектор.

$$
\frac{\partial L}{\partial {w}} \in \mathbb{R}^{n \times 1}
$$

Хазайлтын градиент:

$$\frac{\partial L}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (\hat{p}_i - y_i)
$$

Жингийн градиент бууруулах дүрэм:

$$
{w} \leftarrow {w} - \alpha \frac{\partial L}{\partial {w}}
$$

$$
b \leftarrow b - \alpha \frac{\partial L}{\partial b}
$$

$\alpha$ = сурах эрчим. $\alpha$ нь тогтмол утга бөгөөд сургалтын давталт болгонд жин, хазайлт хоёрын өөрчлөлтийг удирдах коэффициент.

```python
def _compute_gradients(self, X, y, y_pred):
    m = len(y_true)
    # error
    error = y_pred - y_true

    if sample_weights is not None:
        error = error * sample_weights

    # Жингийн градиент
    if issparse(X):
        dw = (1/m) * X.T.dot(error)
        dw = np.asarray(dw).flatten()
    else:
        dw = (1/m) * np.dot(X.T, error)
    
    # + L2
    dw += (self.reg_lambda / m) * self.weights.flatten()
        
    # Bias-ийн градиент
    db = (1/m) * np.sum(error)
        
    return dw, db
```

Сургалт нь дараах нөхцлийн аль нэг нь биелэхэд зогсоно:
1. Сургалтын алдаа тодорхой утга руу нийлэх үед 
2. `max_iter` 1000 давах үед 
3. Баталгаажуулалтын алдаа нь багасахаа болих үед (early stopping) 

## Нэмэлт

### Классын жин

Бидний өгөгдөл нь классын хувьд тэнцвэргүй байсан тул $c \in \{0,1\}: \quad w_c = \frac{m}{2 \cdot m_c}$ зарчмаар классын жинг тооцоолсон. Ингэснээр манай загвар цөөнх класс дээр илүү их ач холбогдол өгнө.

```python
self.class_weights_ = self.compute_class_weights(y.flatten())
sample_weights = np.array([self.class_weights_[cls] for cls in y.flatten()]).reshape(-1, 1)
```

- $m$: түүврийн хэмжээ

- $m_c$: класс {c} -ийн түүврийн хэмжээ

Түүвэр болгоны алдаа нь классынх нь жингээр үржигдэнэ:

$$
\ell_i = - w_{y_i} \big[y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\big]
$$

### Сургалтын эрчмийг багасгах

Сургалтын эрчим нь сургалтын $t$-р давталт дээр дараах зарчмаар багасна:

$$
\alpha_t = \frac{\alpha_0}{1 + \text{decay} \cdot t}
$$

- $\alpha_0$: анхны сургалтын хэмжээ

- $decay$: тогтмол, гиперпараметр

```python
self.learning_rate = self.initial_lr / (1 + self.lr_decay * iteration)
```

Бидний моделийн градиент бууруулалтын дүрэм:

$$
w \leftarrow w - \alpha_t \frac{\partial \mathcal{L}}{\partial w}, \quad
b \leftarrow b - \alpha_t \frac{\partial \mathcal{L}}{\partial b}
$$


### Early stopping

Алдаа нь баталгаажуулалтын өгөгдөл дээр багасахаа болих үед сургалтыг зогсооно. "Overfitting" тохиолдохоос сэргийлэх арга.

```python
if val_loss < best_val_loss:
    best_val_loss = val_loss
    patience_counter = 0
else:
    patience_counter += 1
    if patience_counter >= patience:
        break
```
### Заагийн утгыг оновчлох

Precision, recall, эсвэл F1 үзүүлэлтүүдийг дээд зэргээр ихэсгэх заагийн утгыг олно. Энэ утгыг $[0.1, 0.9]$ интервалаас хайна.

```python
def optimize_threshold(self, X, y_true, metric='f1'):
    probas = self.predict_proba(X)[:, 1]
    thresholds = np.linspace(0.1, 0.9, 81)
    
    best_score = 0
    best_threshold = 0.5
    
    for threshold in thresholds:
        y_pred = (probas >= threshold).astype(int)
        
        if metric == 'f1':
            score = f1_score(y_true, y_pred)
        elif metric == 'precision':
            score = precision_score(y_true, y_pred)
        elif metric == 'recall':
            score = recall_score(y_true, y_pred)
        else:
            score = f1_score(y_true, y_pred)
        
        if score > best_score:
            best_score = score
            best_threshold = threshold
    
    self.threshold = best_threshold
    return best_threshold
```
### Xavier/Glorot initialization

Онцлогуудын жингийн вектороо $0$ утгатай зарлахын оронд $[1; n]$ хүртэлх санамсаргүй утгуудаар дүүргээд $\frac{1}{n}$-р үржүүлнэ. `numpy.random.randn()` функц нь стандарт хэвийн тархалтаас түүврүүдээ үүсгэдэг. $\mu = 0$, $\sigma^2 = 1$. $\frac{1}{n}$-р үржүүлснээр $\sigma^2 = \frac{1}{n}$ болох ба энэ нь градиент бууруулалт болон бусад машин сургалтын алгоритмуудын үр ашгийг нэмэгдүүлнэ. Градиентийг хэт их/бага утга руу тэмүүлэхээс сэргийлнэ.

```python
self.weights = np.random.randn(n, 1) * np.sqrt(1.0 / n)
```

## Үндсэн fit() функц

```python
    def fit(self, X, y, X_val=None, y_val=None):
        if issparse(X):
            m, n = X.shape
            X_array = X
        else:
            X = np.array(X)
            m, n = X.shape
            X_array = X
        
        y = np.array(y).reshape(-1, 1)
        
        self.class_weights_ = self.compute_class_weights(y.flatten())
        sample_weights = np.array([self.class_weights_[cls] for cls in y.flatten()])
        .reshape(-1, 1)
        
        self.weights = np.random.randn(n, 1) * np.sqrt(1.0 / n)
        self.bias = 0
        
        prev_loss = float('inf')
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 10
        
        for iteration in range(self.max_iter):
            self.learning_rate = self.initial_lr / (1 + self.lr_decay * iteration)
            
            if issparse(X_array):
                z = X_array.dot(self.weights) + self.bias
                z = np.asarray(z).flatten().reshape(-1, 1)
            else:
                z = np.dot(X_array, self.weights) + self.bias
            
            y_pred = self.sigmoid(z)
            
            loss = self.compute_loss(y, y_pred, sample_weights)
            self.losses.append(loss)
            
            dw, db = self.compute_gradients(X_array, y, y_pred, sample_weights)
            
            self.weights -= self.learning_rate * dw.reshape(-1, 1)
            self.bias -= self.learning_rate * db
            
            if X_val is not None and y_val is not None:
                val_loss = self._compute_val_loss(X_val, y_val)
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        break

            if abs(prev_loss - loss) < self.tol:
                break
            prev_loss = loss
        
        return self
```

# Үр дүн

## Ерөнхий гүйцэтгэл:

Манай загвар эцэст нь хамааран хувьсагч $y_i$-н утгыг зөв таахад л оршино. Энэ хувьсагч нь $\{0, 1\}$ гэсэн хоёрхон утга авах ба энэ нь түүврийн класс болно. Бид баталгаажуулалтын өгөгдөл дээр $0$ классын түүвэрт зөв таамаглал гаргах тоолонд true negative, харин буруу гаргахад false negative үзүүлэлт 1-ээр нэмэгдэнэ. Мөн адил $1$ класс дээр зөв таах нь true positive, алдах нь false positive үзүүлэлтүүд нэмэгдэнэ.

$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

$$
Precision = \frac{TP}{TP + FP}
$$

$$
Recall = \frac{TP}{TP + FN}
$$

$$
F_1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
$$

Моделын заагийн утгыг F1 утгын хувьд оновчлох үед ерөнхий үзүүлэлтүүд:

- **Accuracy**: $\approx$ 0.82
- **Precision**: $\approx$ 0.60
- **Recall**: $\approx$ 0.74
- **F1 Score**: $\approx$ 0.66

Бидний модел класс 1-н түүврүүдийн хувьд ихэнхдээ зөв таамаглал гаргадаг ч precision = 60% байгаа нь $1$ гэж таамагласан түүврүүдийн 40% нь $0$ байсан гэдгийг илтгэж байна. Мөн recall = 74% байгаа нь нийт класс 1-н түүврүүдийн 74%-г зөв таасан гэдгийг харууллаа. 

## Confusion матриц

![Confusion матриц](images/conmatrix.png)

## Заагийн утга

Дараах графаас заагийн утга болон харгалзах F1, precision болон recall-ийн оноонуудыг харж болно. F1 оноо нь хамгийн ихдээ ойролцоогоор 0.66 гарах билээ.

![Заагийн утга](images/threshold.png)

## Онцлогийн ач холбогдол

Сигмоид функц шуглан нийлбэрийг ($z = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b$) магадлал руу хөрвүүлдэг. 


- $z \to +\infty$, үед $e^{-z} \to 0$, учир:

$$
\sigma(z) \to \frac{1}{1 + 0} = 1
$$

- Харин $z \to -\infty$, үед $e^{-z} \to \infty$, учир:

$$
\sigma(z) \to \frac{1}{1 + \infty} = 0
$$

Жингийн утга нь ихсэх тусам тухайн түүвэр нь класс 1-н гишүүн байх магадлал нь дагаж ихэснэ. Жингүүдийг харахад аль онцлог $x$ хамгийн чухал болохыг харж болно.

![Хамгийн чухал 15 онцлогийн жин](images/features.png)

## Магадлалын тархалт

Зарим $>50K$ орлоготой хүмүүс өндөр магадлалтай (0.9+) гарч байгаа ч, зарим нь харьцангуй бага магадлалтай (0.2 - 0.4) байна. Энэ магадлалын давхцал нь зарим өгөгдөл дээр тодорхой ангилалт хийх бэрхшээлийг харуулж байна.

Зүүн талд математик функц, баруун талд бидний загварын таамаглалууд хэрхэн тархсаныг харуулж байна:

![Сигмоид функц ба магадлалын тархалт](images/sigmoid.png)

*   **Улаан хэсэг ($\leq50K$):** Ихэнх нь $0 - 0.3$ магадлалтай байна. Загвар энэ хүмүүсийг бага орлоготой гэдэгтээ нэлээд итгэлтэй байна.
*   **Ногоон хэсэг ($>50K$):** Тархалт нь $0.2$ - оос $0.9$ хүртэл маш өргөн байна. Энд давхцал их байгааг анзаараарай.

# Дүгнэлт

Энэхүү төсөл нь логистик регрессийг практикт хэрэгжүүлэх явцдаа зөвхөн алгоритмын ажиллагаа төдийгүй өгөгдлийн чанар, статистик ойлголтууд загварын гүйцэтгэлд ямар их нөлөөтэйг бодитоор мэдрэх боломж олголоо. Загвар тогтвортой суралцаж, overfitting ажиглагдаагүй нь L2 арга болон градиент бууруулалт оновчтой байсны илрэл юм. Гэсэн хэдий ч гүйцэтгэл, ялангуяа F1 үнэлгээ нь $0.66$ давахгүй байгаа нь өгөгдлийн бүтэц, классын тэнцвэргүй байдал, оролтын хувьсагчийн хамаарал нь шугаман бус байх зэрэгтэй холбоотой байж болзошгүй.

Ирээдүйд random forest, decision tree зэрэг шугаман бус загваруудыг ашиглавал >50K орлоготой хүмүүсийг илүү найдвартай таамаглах боломжтой гэж үзэж байна. Гол сургамж нь зөвхөн accuracy-аас гадна precision, recall, F1 үзүүлэлтүүдийг чухалчилж, ашиглах загвараа болон өгөгдлийн чанарыг хамтад нь үнэлэх хэрэгтэй гэдгийг харуулж байна.

# Багийн гишүүдийн оролцоо

Багаараа цаг товлон уулзаж, зорилго болон сэдэв сонголтоо хийсэн. Хүн бүр өөрсдийн хийх ажлыг хуваан авч гэр гэрээсээ хийхээр болсон. Github дээр шинэ төсөл үүсгэж 4 гишүүд бүгд жигд оролцон хийсэн.

Төслийн холбоос: https://github.com/JavkhlanGanbat/Statistics

# Ном зүй

- Fatakdawala, M. (2018). Income Dataset. 
  <https://www.kaggle.com/datasets/mastmustu/income>

**Анхаарах зүйл**: Энэ өгөгдлийг анх 1994 оны U.S. Census Current Population Survey судалгаагаар авч, Барри Бекр боловсруулалт хийж, Ронни Кохави UCI ML repository-д хандивласан. 
(https://www.cs.toronto.edu/~delve/data/adult/adultDetail.html).

- Scikit-learn developers. *Logistic Regression: scikit-learn documentation*.  
  <https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression>

- StatQuest. (2024). *Logistic Regression*.  
  <https://www.youtube.com/watch?v=yIYKR4sgzI8&list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe>

- Russell, S., & Norvig, P. (2016). *Artificial Intelligence: A Modern Approach* (3rd ed.). Pearson.

- Google Developers. *Logistic regression: Loss and regularization*.  
  <https://developers.google.com/machine-learning/crash-course/logistic-regression/loss-regularization>

- Danushka. (2019). *Logistic Regression* (lecture notes).  
  <https://danushka.net/lect/dm/logreg.pdf>

- Sanderson, G. (2016). *Essence of Linear Algebra*.  
  <https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab>